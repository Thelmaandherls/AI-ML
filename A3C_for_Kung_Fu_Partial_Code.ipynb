{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thelmaandherls/AI-ML/blob/Udemy/A3C_for_Kung_Fu_Partial_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A3C for Kung Fu"
      ],
      "metadata": {
        "id": "dIo6Zkp7U1Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "pz8ogVxGVB6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "CqN2IEX1VKzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e2d612-4f89-4459-83b7-b1bd567fbe4e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.5)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.8.30)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=daf76b344a62685b7ef8550362efb237c3f000c3d9b5a72d34b429be669e3a10\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,004 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123605 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349114 sha256=dfb25f0c5736f2e457398fec8f54abff43b1b83eee424cea35527ad670d29299\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "BrsNHNQqVZLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "from gymnasium import ObservationWrapper\n",
        "from gymnasium.spaces import Box"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "VF6EFSGUVlk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "qyNc8cxbZCYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Brain(nn.Module):\n",
        "#network class enables a brain and eye of ai to be formed\n",
        "\n",
        "  def __init__(self, action_size):\n",
        "    #activate the Network calss with the super fucntion\n",
        "    #network calss wil inhertiyt ftom the network module\n",
        "    super(Brain, self).__init__()\n",
        "    #define the conlution operatiosn, created as an instance of the conv2d class imported from nn module from torch library\n",
        "    #will have 4 grey scale frames from kungfu env\n",
        "    #dcn - besy to inc the num of filters per conv but in a3c it's fine to not icn\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=4, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "    self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2)\n",
        "    #creating the flattenign layer wothin an obj self.flatten\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    #fully connected layers\n",
        "    self.fcl1 = torch.nn.Linear(512, 128)\n",
        "    #moving directly to final output layers - the action values - the sum of rewards starting from the current states - in dqn and dcn\n",
        "    #also have the state value - single output of the network that provides an estimate of the value of the current state\n",
        "    #value of current state = prediction of the epxetcc return from the current state if the agent follows the current polcity\n",
        "    #this will output the action values / q values for each of the action represnign ghe expected return if you play the action fromt he cirrent state\n",
        "    self.fcl2a = torch.nn.Linear(128, action_size)\n",
        "    #state value - excteing only 1 state value so outptu will be 1\n",
        "    self.fcl2s = torch.nn.Linear(128, 1)\n",
        "\n",
        "#forward propr sig from input images to final 2 output layers of the action values and state value\n",
        "#state = input frames\n",
        "  def forward(self, state):\n",
        "    x = self.conv1(state)\n",
        "    #use relu to activate the sig using the rectivfier activation func\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    #forward prop from conv layer to flattenig layer\n",
        "    x = self.flatten(x)\n",
        "    #forward propr to only intermediary layer\n",
        "    x = self.fcl1(x)\n",
        "    #have to activate the sig forwrdprop from flatte to fcl1\n",
        "    x = F.relu(x)\n",
        "    #forward propr to 2 final output layers\n",
        "    action_values = self.fcl2a(x)\n",
        "    #use index 0 to get the state value itself not in any vector or arrya\n",
        "    state_value = self.fcl2s(x)[0]\n",
        "    return action_values, state_value"
      ],
      "metadata": {
        "id": "wV1mCZWMACb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "eF5bETqbZbCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "3C2ydyKLZgaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessAtari(ObservationWrapper):\n",
        "\n",
        "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
        "    super(PreprocessAtari, self).__init__(env)\n",
        "    self.img_size = (height, width)\n",
        "    self.crop = crop\n",
        "    self.dim_order = dim_order\n",
        "    self.color = color\n",
        "    self.frame_stack = n_frames\n",
        "    n_channels = 3 * n_frames if color else n_frames\n",
        "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
        "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    self.frames = np.zeros_like(self.frames)\n",
        "    obs, info = self.env.reset()\n",
        "    self.update_buffer(obs)\n",
        "    return self.frames, info\n",
        "\n",
        "  def observation(self, img):\n",
        "    img = self.crop(img)\n",
        "    img = cv2.resize(img, self.img_size)\n",
        "    if not self.color:\n",
        "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = img.astype('float32') / 255.\n",
        "    if self.color:\n",
        "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
        "    else:\n",
        "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
        "    if self.color:\n",
        "      self.frames[-3:] = img\n",
        "    else:\n",
        "      self.frames[-1] = img\n",
        "    return self.frames\n",
        "\n",
        "  def update_buffer(self, obs):\n",
        "    self.frames = self.observation(obs)\n",
        "\n",
        "def make_env():\n",
        "  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n",
        "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
        "  return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "state_shape = env.observation_space.shape\n",
        "number_actions = env.action_space.n\n",
        "print(\"Observation shape:\", state_shape) # 4 layers of a swaure 42x42\n",
        "print(\"Number actions:\", number_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF756uIhRVcK",
        "outputId": "42651265-bb77-4905-8b06-4ddbedc87176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation shape: (4, 42, 42)\n",
            "Number actions: 14\n",
            "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_action_meanings to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_action_meanings` for environment variables or `env.get_wrapper_attr('get_action_meanings')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "YgRlooBmC1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparamter - lr  of atom optimiser to update module paramter / weights of the nn as back propr info\n",
        "learning_rate = 1e-4\n",
        "#hyperparam used in formual to compute target state value\n",
        "discount_factor = 0.99\n",
        "#hyperparam - trianing xx agent sin xxx envs in parallel whch are running their own proccesses inside with indepent agnts running their own env\n",
        "number_environments = 10"
      ],
      "metadata": {
        "id": "fBiW2Sry_70Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the A3C class"
      ],
      "metadata": {
        "id": "Gg_LmSs9IoTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inetgrating the leanr method into the step method\n",
        "class Agent():\n",
        "\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "    self.Brain = Brain(action_size).to(self.device)\n",
        "    #tool to update the models prama / weight sof thebrain in the nn when backprop the loss through the nn\n",
        "    #whenever using an optimsier, had to consider the lr\n",
        "    self.optimizer = torch.optim.Adam(self.Brain.parameters(), lr = learning_rate)\n",
        "\n",
        "  #implemnt act method -> agent can implent an action in a certain state using softmax strtegy / ploicy\n",
        "  def act(self, state):\n",
        "    #state needs to be in a batch as it's what nn in pytorch expects\n",
        "    #ndim = dimension\n",
        "    #==3 - states are a 4 frame buffer - stack of 4 gray scale dimension - 1 simension for which fram ein the 4 frames, and then the other 2 dimension for the 42x42\n",
        "    #if 3 = not in a batch thf single dimension sclae\n",
        "    if state.ndim == 3:\n",
        "      #this ensures state is in a batch and thf makesit 4 by adding na extra dimension\n",
        "      state = [state]\n",
        "    #convert state to torch tensor\n",
        "    state = torch.tensor(state, dtype= torch.float32, device = self.device)\n",
        "    #get the action vqalues only thf use , and _ to not include state valyes\n",
        "    action_values, _ = self.Brain(state)\n",
        "    #softmax ploicy to selct a specific action\n",
        "    #convert action values into proabilitesi - highest will be used\n",
        "    #dim=-1 - softmax should be applied across the last dimension\n",
        "    policy = F.softmax(action_values, dim = -1)\n",
        "    #state is a batch of sevral state so goin to return batch of several actions each corrsponsibding with specific states in batch\n",
        "    #plociy is also a list of xx polidies for each state in the batch\n",
        "    #loop over each ploicy for eahc state within a batch in the list\n",
        "    #going to loop over one specifi state in the batch and then repeat for other batches\n",
        "    #choice func - enables distribution of probabilities to be drawn out\n",
        "    #len (p) = number of actions\n",
        "    #p=p is 1 policy coreesponding to a specific state in the batch of the policy tensor\n",
        "    #looping over each policy\n",
        "    #.detcah to detcah policy tenspr from computational graphs as won't be using gradients, send to cpu and convert tesnor to np array\n",
        "    return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
        "\n",
        "#step method- method to be called when the agent takes a step in the env - recives xx vars which then update the model params/weight of nn -> trainign -> better score\n",
        "#the argument passed in this method are batches in numpy arrays\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    batch_size = state.shape[0]\n",
        "    #1st dimension in a torhc tensor reps #states of obsv\n",
        "    #converting arguments from np arrays to torch tensors\n",
        "    state = torch.tensor(state, dtype= torch.float32, device = self.device)\n",
        "    next_state = torch.tensor(next_state, dtype= torch.float32, device = self.device)\n",
        "    reward = torch.tensor(reward, dtype= torch.float32, device = self.device)\n",
        "    #done was already a bool alue - done or not - have to ensure all values are in the same format to change it to a float format at the end\n",
        "    done = torch.tensor(done, dtype= torch.bool, device = self.device).to(dtype= torch.float32)\n",
        "    action_values, state_value = self.Brain(state) #batch of states\n",
        "    #need next state value for deep q learning and not the next action values\n",
        "    _, next_state_value = self.Brain(next_state)\n",
        "    #computing target state value using the bellman equation\n",
        "    target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
        "    #advatnge part of a3c\n",
        "    advantage = target_state_value - state_value\n",
        "    #ciritic of a3c - computing total loss which will back prop into nn - total loss = critic loss - actors loss (adv = [prt of actors loss])\n",
        "    #req entropy to compute actor loss\n",
        "    #req distirbution of probabilities over the action values and the log probablities over the same action values to calc entropy\n",
        "    #probs = same as the ploicy in the act memehtod within this class\n",
        "    probs = F.softmax(action_values, dim = -1)\n",
        "    log_probs = F.log_softmax(action_values, dim = -1) #tensor\n",
        "    #entropy is the negative sum of probs and logprobs - axis = axis dimension in which doing the sum in\n",
        "    entropy = -torch.sum(probs * log_probs, axis = -1)\n",
        "    #now need log probs of actions that are actually selected from the whole bacth of states\n",
        "    #creating an array of batch sizes and then using it to select th elog probs of the actions taken in the bacth\n",
        "    #.arange - dtermines the size of the array - here i say that the size of the array is the size of the batch_size\n",
        "    batch_idx = np.arange(batch_size)\n",
        "    #using batch_idx to slect log pobs of actions actually playes\n",
        "    #calling logprobs and then accessing the indexes of actions taken by the batch which are given by the action var\n",
        "    #takign all the rows in the logprobs tensor to get all of the log probabilities ofr each action selected by entering batch_idx as the first dimension\n",
        "    logp_actions = log_probs[batch_idx, action]\n",
        "    #actors loss = -ve mean of the product of the log probs of the actions taken x the advs\n",
        "    #simultaneously encouraging exploration by adding the mean of entropy x a hyperparam (0.001) that will balance the improtance of entropy\n",
        "    #adding .deatch to adv tensor to prevent gradients from flowinf into the citic network durng the actors update\n",
        "    actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()\n",
        "    #criticis loss = mean swaured error loss b/w tareget state and actual state value\n",
        "    critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    #backprop total loss into a3c nn and then use optimiser to update nn weights\n",
        "    #have to reset optimiser first ebfore bakc prop ALWAYS\n",
        "    #zero_grad method resets te optimsier by zeroingin out the gradients to prevent acumualtion from previous iterations\n",
        "    self.optimizer.zero_grad()\n",
        "    #back prop total loss\n",
        "    total_loss.backward()\n",
        "    #update nn weight with optimsier - .step updates the weight of nn to minimise total_loss\n",
        "    self.optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kfg6ir5t_8Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the A3C agent"
      ],
      "metadata": {
        "id": "7RnRukHDKFJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(number_actions)\n",
        "#takes action size which is num of actions from the preprocess atari class which is 14"
      ],
      "metadata": {
        "id": "d8noU45p_-oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our A3C agent on a single episode"
      ],
      "metadata": {
        "id": "oB5SpmoKP0aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the a3c agent on a specific num of eps  - will ahe the numof eps as a hyperparam\n",
        "#evaluate func returns a list of total rewards of each number of action in a specific num of episodes\n",
        "def evaluate(agent, env, n_eps = 1):\n",
        "  #initlaised as empty list but will contain total accumkated rwards for each epsthe agent is evaluated on\n",
        "  episodes_rewards = []\n",
        "  #use _ as not using loop var - looping over diff eps\n",
        "  for _ in range(n_eps):\n",
        "    #staring a new ep so need to initialise the state using reset method from env object - only want state\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    #doing an infinite loop that will break if the epsiodeis done\n",
        "    while True:\n",
        "      #dealing w a specific ep within a specific time step, so know the agent needs to play an action to reach the next state\n",
        "      action = agent.act(state)\n",
        "      #getting the next state which si the state the agnet reaches after laying the actions, the reward, dones, info and other var - callstep emthod which takes index 0 as action is an np.array and take index 0 as assume evaluting agent in non-batch mode\n",
        "      state, reward, done, info, _ = env.step(action[0])\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    #let the while loop, now appending the total_reward from the whle loop to the epsioddes reward within the for loop\n",
        "    episodes_rewards.append(total_reward)\n",
        "    #evaluate func has to return list of certain rewards over a specific number of epsidoes\n",
        "  return episodes_rewards\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ecWudoAw__f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing multiple agents on multiple environments at the same time"
      ],
      "metadata": {
        "id": "jVSqiyjiQeMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a class to handle multiple env simultaneously in the context of a parallesized reinforcement learning - what a3c algo is about\n",
        "#this can be done for exmale by faiclitating the simualtneous steppign and ressting of xxx env to use aa3c env more readily\n",
        "class EnvBatch:\n",
        "  #n_envs = num of env tomanage simualtenously - hard cording so instance will always be 10 envs\n",
        "  def __init__(self, n_envs = 10):\n",
        "    #list of xx env\n",
        "    #use make_env func through a for loop within a list to create xxx envs in one var\n",
        "    #_ = loop varibale that is disregarded as will not be used explicitly\n",
        "    self.envs = [make_env() for _ in range (n_envs)]\n",
        "\n",
        "  #reset method to reset xxx envs at simultaneously thf can have xxx management of env\n",
        "  def reset(self):\n",
        "    #list of initalsied state that is gotten from self env obj containing diff envs\n",
        "    _states = []\n",
        "    #create list of reset/initlaised states for each env in self.envs - done with for loop to loop over diff env in self.envs - env = env wihin xx envs\n",
        "    for env in self.envs:\n",
        "      #getting initialsied states in each of the self.env using the reset method - reset method returns more than the statesso use index 0\n",
        "      _states.append(env.reset()[0])\n",
        "      #return list of initalised list in form of np array\n",
        "    return np.array(_states)\n",
        "\n",
        "  #method that enables us to step in xx env simul\n",
        "  #take actions as gotten to that state based on action - doinf xxx agent thdf ahve xx actions\n",
        "  def step(self, actions):\n",
        "    #trasnforming method from single step in 1 env to  xx envs\n",
        "    #doing a double loop action w var a called in step and also in the for loop so get 1 action per env\n",
        "    #zip func finsihes the double loop - loop in the self.env xx var obj and also actions\n",
        "    #have to return as np array thf do zip at the start to transpose list of tupples / to chnage to format of np array\n",
        "    #have to add * to do the transposition -can't directly use np array func as usezip and transpose so have to use map func called onnp.array\n",
        "    #this line of code gets the group of next states, rewards, ddones and info all converted into np arrays\n",
        "    next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
        "    #have to chekc if any of the xx envs have finished, if it has, then it needs to be reset\n",
        "    #len(Self.envs) to see the number of envs\n",
        "    for i in range(len(self.envs)):\n",
        "      #if dones boolean of env i is true then reset states of env which is given by [i]\n",
        "      if dones[i]:\n",
        "        #next state of specific env, now goign to reset it by updating it - calling reset func from specific env, accessed by self.envs obj as it contains xx envs\n",
        "        #index 0 to get onnly the state\n",
        "        next_states[i] = self.envs[i].reset()[0]\n",
        "    return next_states, rewards, dones, infos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6AY3PSnbAAJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the A3C agent"
      ],
      "metadata": {
        "id": "69WZWB4oRx1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm #module displays progress bar of the loops\n",
        "\n",
        "#instance of the env bacth class which will be 10, thf have 10 agents being changed in 10 diff envs -> better and faster trainign env that reaches traning very fast\n",
        "#using num envs and no n-envs as n_envs = name of arg\n",
        "env_batch = EnvBatch(number_environments)\n",
        "#reset batch of states - have batch of states in c=xx envs from env batch class - use reset method called foem insatnce of env batch class\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "#growing progress bar for training using t.range func in tqdm module - contians rnage of iteratiosn the mdoel will be trianed in - include upper bound as prinving avg score every 10 eps\n",
        "with tqdm.trange(0, 3001) as progress_bar:\n",
        "  for i in progress_bar:\n",
        "    #batch of state has been reset - now multiple agents in xx envs - eahc agent needs to play an actions in the intilaised sate simultaneously usng act method\n",
        "    batch_actions = agent.act(batch_states)\n",
        "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
        "    #xx env se=tep method for xx actions in xx envs\n",
        "    #rl trick to stabilise the trianing - dec th emagntiude of rewards to stabilise the trianing\n",
        "    batch_rewards *= 0.01\n",
        "    #calling step method from agent class which performs step of the trainign and will compute the total loss and then back rop with atom optimiser\n",
        "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
        "    batch_states = batch_next_states\n",
        "    #printing avg reward every 100 iteratiosn so progress bar wil grow 3 times\n",
        "    if i % 1000 == 0:\n",
        "      #compute acucmulated reards over specific num of eps and then get the mean of the output of the evlaute func - every 10 eps\n",
        "      print(\"Avergae agent reward: \", np.mean(evaluate(agent, env, n_eps= 10)))\n",
        "\n"
      ],
      "metadata": {
        "id": "eCFiAzepABDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665ac39e-35d6-41a8-c8a6-64896cdc6730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n",
            "  0%|          | 0/3001 [00:00<?, ?it/s]<ipython-input-41-d6752d39ac73>:79: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
            "<ipython-input-41-d6752d39ac73>:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  state = torch.tensor(state, dtype= torch.float32, device = self.device)\n",
            "  0%|          | 5/3001 [00:44<5:31:04,  6.63s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avergae agent reward:  500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1005/3001 [02:01<1:41:02,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avergae agent reward:  870.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 2005/3001 [03:31<54:03,  3.26s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avergae agent reward:  1750.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3001/3001 [04:50<00:00, 10.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avergae agent reward:  710.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "7kG_YR9YdmUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  frames = []\n",
        "  while not done:\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _, _ = env.step(action[0])\n",
        "  env.close()\n",
        "  imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, env)\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "UGkTuO6DxZ6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}