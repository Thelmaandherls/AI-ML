{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thelmaandherls/AI-ML/blob/Udemy/Deep_Q_Learning_for_Lunar_Landing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZcI9ZXHl3a"
      },
      "source": [
        "# Deep Q-Learning for Lunar Landing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yPRjteXgPb"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEm5teGWjWU"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee23f3e6-6d6e-49d7-9750-1e1570d184f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.8.30)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqiMN3UW9T9"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZaKXP_aMl9O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlDKXvkXzGI"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtG6Zc83YYy3"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Brain(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed = 42): #state_size is 8 but will be speciiefd later in trianining, action size is 4 and seed is 42\n",
        "      super(Brain, self).__init__() #aim of this row is to activate the inheritance\n",
        "      self.seed = torch.manual_seed(seed) #geenrate vectors\n",
        "      self.fcl1 = nn.Linear(state_size, 64) #first full connetion between input layer ad the first fully conencted later- state_size = #neurons in the input layer and 64= optimum number of neurons in the first fully connected layer to build a fully fucntional AI via actual calcs\n",
        "      self.fcl2 = nn.Linear(64, 64) #if i was to have just 1 fcl then the #would be 4 as that si the numebr of the output, but to have another fcl, the best #neruons si 64\n",
        "      self.fcl3 = nn.Linear(64, action_size) #4 = action size\n",
        "\n",
        "  #building the forward method inside the Brain class which will forward-propaagte the signal from input to output layers through both fcl\n",
        "  def forward(self, state): #called it state as it's going to forward-prop a sig from the state to the output layer to playy a specific aciton at a specific time\n",
        "      x = self.fcl1(state) #propagate sig from input to fcl1 - use fcl1 obj as it's an insatcne fo the input class and retirn the first full conenction - it takes the state as a current time step in the env - as fc1 state returns the first ully conencted layer, create a new var x to contain what is returned by self.fc1(state)\n",
        "      x = F.relu(x) #activate the sig thanks to a rectifier activation function by updating the value of x but calling first the F short from the improted functional pytorch module and call the reluc func which is the rectifier activation func\n",
        "      #these 2 lines of progapte the sig from the input to the fc1 with a rectifier activation func and then do the same from the fcl2\n",
        "      x = self.fcl2(x) #fc2 takes as input the sig from the fc1 so use x and not state\n",
        "      x = F.relu(x)\n",
        "      return self.fcl3(x)\n",
        "      #final sig in the output layer containing actions\n",
        "      #this ofrward props the states form input to output\n",
        "      #pytorch can create the architcture efficiently"
      ],
      "metadata": {
        "id": "HqpT4ZEKSi2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVrBnFWZKb1"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T364fz9qZb2j"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialising vars"
      ],
      "metadata": {
        "id": "InMEbiHVpVBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('LunarLander-v2') #import env ai will be trained\n",
        "#paramteres I will req later for training\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0] #number of elements at easch inputs - the cordinates describing the states\n",
        "num_actions = env.action_space.n\n",
        "print(\"State shape:\", state_shape) #vector of 8 elements\n",
        "print(\"State size:\", state_size) # 8 input values in each state\n",
        "print(\"Number of actions:\", num_actions) # 4 actiosn that can be executed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv86gD2xD50P",
        "outputId": "2d58632c-b8d6-47ac-e1a7-15d11c6a1f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape: (8,)\n",
            "State size: 8\n",
            "Number of actions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_dZmOIvZgj-"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4 #best value after exps\n",
        "minibatch_size = 100 #num of osebcs used in 1 step of the trianing to update mdoel parameters - use this as the usual value for DQL\n",
        "discount_factor = 0.99 #represents values of rewards - close to 1 = better cnsideration of future rewards\n",
        "replay_buffer_size = int(1e5) #size of the memroy of the ai- the number of exepriences, rewafds, mixed states and action performed - toimprve trianing process\n",
        "interpolation_parameter = 1e-3 #interpolation prameter used for training - used for sub update - best value after exp"
      ],
      "metadata": {
        "id": "r-Qe5zT9GUNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hD_Vs-bYnip"
      },
      "source": [
        "### Implementing Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "  #use this class name typically for class that inplements expeienrce replay and use noinheritance as use bject\n",
        "  def __init__(self, capacity):\n",
        "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #usful if want to make faster and moreperformant usogn a gpu\n",
        "      self.capacity = capacity #max size of memory buffer\n",
        "      self.memory = [] #list that will store expeience, each one including state, reward, next state done ornot - initlaise this to an exmpty list\n",
        "      #this si the ocntrutor method that initalises the replay memory obj\n",
        "\n",
        "#adding a new method into the replay memery class which will add new expeirnce into the replay memeory class without overlaoding the capcity\n",
        "#experience = current state, fuure state, reward, action, boolean saying if action is done or not\n",
        "  def push(self, experience):\n",
        "    self.memory.append(experience) #append an event to this memory\n",
        "    #ensure ememry buffer does not exceed capcity\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0] #removing the oldest memory\n",
        "\n",
        "  #sample method that will randomly selct a batch of experiences from the memory buffer\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k = batch_size)\n",
        "    #going to extract the diff elemts of eahc smaple fo expeience indiviually\n",
        "    #1 row for loop = for loop within a list ([]) - then stack the first elemtns within those experiences as long as they exist\n",
        "    #convert these stack of states into pytorch tensors via torch library = torch.from_numpy()\n",
        "    #ensure that the datate type is float to conserve the format expected by pytorch\n",
        "    #.to(Self.device) = to send back to the designated computign device\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device) #.long = long int as action can be 0,1,2,3\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device) #uint8 converts to bool which si then converted to float\n",
        "    return states, next_states, actions, rewards, dones\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EspRBJNkHVlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmEkbFbUY6Jt"
      },
      "source": [
        "### Implementing the DQN class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating an agent class that defs behvaiour of agent that inetrcat with space env via dqn - whislt interacting w env, agent maintian 2 nws: local and target\n",
        "locla = selects action\n",
        "targte = calcs taregt q values that will be used to trian local qnw\n",
        "this double 1nw setup will staebiise learning process in learn method\n",
        "softupdate method will update target qnw params by blending them with those of the locla q nw using the softupdate formuale - this prevents abrupt changes that could destabilsie the trianing\n",
        "implemented act agent that wil help agent choose actions based on current understanfinf of optimal policty -will be returned from local qnw that will fwd prop the state to obtain action values and then use the epsiolon greedy policty to resturn final action\n",
        "eg strategy is used as expiration mech - fact tahts oemtimes can selct random actions enables agent to explore more actions that could possibly eb more beneficial\n",
        "learn method uses experiences samples from replay ememory to update local qnw q values towards the target qnw"
      ],
      "metadata": {
        "id": "v_QxZeArm6jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Brainiac():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    #choose if want to use the cpu or gpu\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #defining these vars according ot the obj var and initlaising them to the argument that wiull be entered later on when creaitng an obj in the instnce of this brainian class\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size #size of action space\n",
        "    #doing Q-learning - creating local and target networks\n",
        "    #create 2 instance of previously madenetwarok - 1 that will be local and the other will eb target\n",
        "    self.local_qnetwork = Brain(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = Brain(state_size, action_size).to(self.device)\n",
        "    self.optimiser = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate) #will update step by step to optimsie actions - create as nstacne of adam clas that will enbale us to use the  adam optimiser and call an attribute of the local qnetwork, parmaters\n",
        "    #to implement the dql class\n",
        "    self.memory = ReplayMemory(replay_buffer_size) #replay_buffer_size = capcity\n",
        "    self.t_step = 0 #counter\n",
        "\n",
        "#method to store expeirneces and decdie when to learn from them\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    #store experience in replay memeory\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    #increment the time stem counter sekf.t_step and reset every 4 steps so we can leanr every 4 step s\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    #learning which will happen every 4 steps - learn base don mini batches\n",
        "    if self.t_step == 0:\n",
        "      #check if #of expeirences in memory is already larger than mini batch size fof 100\n",
        "      #using self.memory instance of the replay memory class and calling the atrribute self.memory from the replaymemory class\n",
        "      if len(self.memory.memory) > minibatch_size:\n",
        "        #using smaple method from replay memory class which will sample 100 exps from the memory\n",
        "        experiences = self.memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "#method to select an action based on a given state and a certain epislon value for an epislon greedy action selction policy\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    #state os a numpy array so want it to be a torch tesnor to work with nn\n",
        "    #use .unsqueeze - add an extra dimension to state vector - currently have 8 dimensions coresponding to coords os spaceship\n",
        "    #this dimension says which batch the state belongs to for each state\n",
        "    #we're using the index of 0 so the first dimension of new state vector / now torch tensor will be at beginning\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    #forward pass state through local  network to get action values, goign to set locla q network to evalaution mode\n",
        "    self.local_qnetwork.eval()\n",
        "    #ensure in inference mode (make predictions) - use no_grad to make sure any gradient computation is disabled\n",
        "    with torch.no_grad():\n",
        "      #make the predictions from the lcoal q network to get the action values/q values corresponding to actions\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    #going back into training mode via .train()\n",
        "    self.local_qnetwork.train()\n",
        "    #epsilon greedy action selection\n",
        "    if random.random() > epsilon:\n",
        "      #select action with highest q value and in numpy array\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      #will select a random number between index 0-3\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "#learn memthod to update agents q values based on sample expereinces\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    #unpack smaple experiences into theit repective vars - upacking the tuple that makes up the experiences\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "    #get max predicted q value fron next_state from target nn/propaagting the next state from target qnw\n",
        "    #because we want to compute q targets for the current state which req maxnq values from next state in formula\n",
        "    #use max of action vaues propapagted through next states - need max value along dimension 1 which corrsponds to action value along the tesor\n",
        "    #after getting max 1 we get 2 tensors: tesnor of max values and tensor of indivies corrsponding to actiosn of these max values\n",
        "    #.unqueeze - need to add imenrion of the batch at position 1\n",
        "    #detach- detaches resulting resnor from computation graph so won't be tracking gradient from backpropagatoon\n",
        "    #this iverall gives the max currated q values from the next states from the target network\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    #computing q taregts for the current states - formual that is used often\n",
        "    q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "    #forward prop the states from the local q network/expected q values from local qnetwork\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    #compute loss between expected and target q values - mean swqusred error losss\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    #initalise the optimiser by restting it / making it 0\n",
        "    self.optimiser.zero_grad()\n",
        "    #back prop the loss to compute the gradient of the closs with respoect to the model params\n",
        "    #this update model params, to update q values -> better action slection poliity\n",
        "    loss.backward()\n",
        "    #singlpe optimisation step to update model params\n",
        "    self.optimiser.step()\n",
        "    #update target nw params with thoseof the local nn\n",
        "    #always use interpolation para, fro soft update\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "#implement soft_updatemethod- method that ill softly update the target network's aparm\n",
        "#soft update: softly updating the target model params using weight avg of local and target params\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    #loop through oarams of local and target\n",
        "    #zip = get params of local and target at the same time\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      #use .copy func to update pparams of target qnw - in brackets = formula of softupdate fo target param\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n"
      ],
      "metadata": {
        "id": "BZqDwU9iMEim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tZElccZmf6"
      },
      "source": [
        "### Initializing the DQN agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creatign an instance of the branianc class\n",
        "not inputting action_size as when intialled the vars in the setting up env, it was aclled number of actions"
      ],
      "metadata": {
        "id": "s8hVBAjlpy3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brainiac = Brainiac(state_size, num_actions)"
      ],
      "metadata": {
        "id": "ppYY5cV4MGwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8v0PtUfaVQp"
      },
      "source": [
        "### Training the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initlais etrianing params\n",
        "#max num of epsidoes we want to train agent but tend to get good reward from abotu a couple 100s\n",
        "num_epsiodes = 2000\n",
        "#max num of time steps per episode - if ai is staying stuck then it can restart\n",
        "maximum_num_timesteps_per_episode = 1000\n",
        "#series of 4 corrsponding to hyepr params correlating to epsiolon greddy action selction policity\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "#decrement - multplyinh it by very close to 1 flaot soit decays by very little  until reach 0.1\n",
        "epsilon_decay_rate = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "#window of scores of 100 epsidoe - initlaise with a double ended q\n",
        "scores_on_100_eps = deque(maxlen = 100)\n",
        "\n",
        "#step where ai has specific intelligence to land on the moon\n",
        "#upper boudn fo range is excluded in py so add 1 to include it\n",
        "for episode in range(1, num_epsiodes + 1):\n",
        "  #always inatrinaing loop, first step is to reset env to intial state at every ep\n",
        "  #.rest returns intial state and other info e.g. intial observation which is discarded by the undeerscore\n",
        "  state, _ = env.reset()\n",
        "  #initlaise the score/ accumualted reward\n",
        "  score = 0\n",
        "  #looping time steps over the eps\n",
        "  for t in range(maximum_num_timesteps_per_episode):\n",
        "    #slecting an action - use .act to slect action in agiven state of env follwing eg policy\n",
        "    action = brainiac.act(state, epsilon)\n",
        "    #making a new move -> ai ends in new state and therefore needs a reward\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    #one of th emost important steps\n",
        "    #tool that will eprform the training\n",
        "    #step method was implemented in agent claass which includes the leanr memehtod\n",
        "    #learn method performs trianing by back prop loss b/w expcted q and target q values with optimiser and on smaples experiences\n",
        "    #this line of code trians agent to land better on the moon\n",
        "    brainiac.step(state, action, reward, next_state, done)\n",
        "    #update state var so it becomes next stae\n",
        "    state = next_state\n",
        "    #score isaccumualted score\n",
        "    score += reward\n",
        "    #check if ep is done\n",
        "    if done:\n",
        "      break\n",
        "  #score of last ep just finished\n",
        "  scores_on_100_eps.append(score)\n",
        "  #decay eg value incrementally whilst ensuring ti doesnt' go beyond ending eg\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_rate * epsilon)\n",
        "\n",
        "#getting the avg score for each ep in dynamic way\n",
        "#ep by ep will see avg of accumualted reward evolving over the eps\n",
        "#but make over wrting effect so each line printed will be removed to give place to the enxt one\n",
        "#retain avg reward over 100 eps after every 100 eps\n",
        "#\\ - adds space\n",
        "#tAverage score = accumualtive reward over last 100 eps unless havent reach 100s eps thne from 0 to this one\n",
        "#:.2f - want avg to be rounded to 2 decimal places\n",
        "#episode = #eps\n",
        "#avg score of var = np.mean(scores_on_100_eps) - as long as haven't rached 100 eps, the scores_on_100_epswill contain less than 100 eps and thf get mean of <100 eps\n",
        "#\\r = carriage returing programme- cursor willr etunr to start of the line when prinitng so in a loop, it will allow newly printed line to override previous -> dynamic effect in console\n",
        "#end = after printing this lien, won't automaticlly create new line which is vital to retain the dyamic programming of \\r\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_eps)), end = \"\")\n",
        "  #check if we are every 100 epsidode using modulo operator to check the rest of the euclean division is 0\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_eps)))\n",
        "  #ep = solution if scores at least 200 points = winning morale - form lunar docu\n",
        "  if np.mean(scores_on_100_eps) >= 200.0:\n",
        "    #:d = will be double inter\n",
        "    #espisode - 100 - started winiing from this number - 100 as this is the score over 100 eps - could keep as just episodes\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_eps)))\n",
        "    #save it's params\n",
        "    #.state_dict = method that will retunr us a dictionary of the model params\n",
        "    torch.save(brainiac.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "id": "In_HjGhOMHpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a622c4-ab1a-4955-fc70-761c99bbe97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -159.36\n",
            "Episode 200\tAverage Score: -117.98\n",
            "Episode 300\tAverage Score: -26.86\n",
            "Episode 400\tAverage Score: 104.15\n",
            "Episode 500\tAverage Score: 174.21\n",
            "Episode 600\tAverage Score: 95.62\n",
            "Episode 700\tAverage Score: 150.17\n",
            "Episode 800\tAverage Score: 182.98\n",
            "Episode 814\tAverage Score: 200.53\n",
            "Environment solved in 714 episodes!\tAverage Score: 200.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CNwdOTcCoP"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(brainiac, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = brainiac.act(state)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(brainiac, 'LunarLander-v2')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}